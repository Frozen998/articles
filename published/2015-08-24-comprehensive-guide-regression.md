#数据分析员不得不知的7种回归技术

##简介

在预测模型中，人们最先接触与学习的算法往往是线性回归与logistic回归。由于它们的运用范围广泛，很大一部分数据分析员们往往疏于思考，而认为所谓回归就只有这两种形式。或许，稍加思考后他们会发现线性回归或logistic回归仅仅是回归分析大家族中最为重要的成员之一。事实上，这世上有数不计数可行的回归形式，而每一种回归形式都具有自己的重要性，适用于自己不同的特殊领域。在本文中，将向大家简要介绍最数据分析常用的7种回归形式，希望它能给你启发，帮助你摆脱每日祈祷线性或logistic回归能适用于每一个所遇到问题的窘境。

![](http://static.datartisan.com/upload/attachment/2015/08/8b5DSJ5u.jpg)

##什么是回归分析？

所谓回归分析，是一种以研究因变量（预测目标）与自变量（预测因子）之间的相关性为目的的预测建模技术，这种技术主要用于预测模型构建、时间序列建模以及分析变量之间的因果关系等方面。例如，分析司机违规行驶次数与交通事故发生数之间的关系就是一个可应用回归分析的经典案例。

毫无疑问，回归分析是数据分析与建模中的一种重要工具。在这种分析中，我们往往会通过曲线的形式对一系列的观测值进行拟合，并同时尽量使得原始观测值与曲线的距离最小化，在下面的章节中，本文将对该方法进行进一步地解释。

![](http://static.datartisan.com/upload/attachment/2015/08/HyUTKZIS.png)



##我们为什么要使用回归分析？

正如上文所提及的，回归分析的目的是对两个或多个变量之间的相关性进行估计，对此，我们可以通过一个简单的例子来了解一下：

比如说，你想要基于最近的经济形势估计一个公司的销售额增长，而你所具有的公司近期数据显示，该公司的销售额增长大约为经济增长2.5倍。通过这样的逻辑，我们就可以运用当期或过去的信息来对这家公司未来的销售额进行预测了。

通常，使用回归分析能够给我们带来许多如下好处：

它能发掘出因变量与自变量之间的显著相关性；
它能显示出众多自变量对一个因变量的影响程度。
通过回归分析，我们还能够比较不同尺度变量间的影响，比如价格变动和促销活动的数量。这些好处有助于市场研究人员/数据分析/数据科学家对自变量进行精简，从而评估出最佳的一组变量用于构建预测模型。



##共有多少种回归分析技术？

可以用于预测建模的回归技术数不胜数，它们大多通过3个因素加以区分：自变量个数、因变量类型、回归线的形状。在下文中，我们将就此进行详细的讨论。

![](http://static.datartisan.com/upload/attachment/2015/08/LMhFf4qP.png)

如果你具有足够强的创新意识与能力，并且在上述3个因素中发现了某种人们从未使用过的组合形式，那么你完全可以研发出一种全新的回归形式，但在此之前，还是让我们先来看看那些最为常用的回归形式吧：



###1.线性回归

线性回归是最为多数人所熟识的建模方法之一了，在人们学习预测模型的构建时，线性回归往往会是首选的几种方案之一。在这项技术中，因变量通常是连续型的，自变量则可以是连续型或离散型的，这种回归拟合的曲线在本质上是线性的。

大多数情况下，线性回归使用一条拟合程度最高的直线（回归线）来反映一个因变量（Y）与一个或多个自变量（X）之间的关系。它可以由方程Y=a+b*X + e来表示，其中a表示截距，b表示直线的斜率，而e则代表随机误差项，根据给定的预测因子数值，可以运用该方程来对模型的目标变量进行预测。

![](http://static.datartisan.com/upload/attachment/2015/08/rCc7s05D.png)

线性回归被分为一元线性回归与多元线性回归，二者的区别在于，多元线性回归拥有两个及以上的自变量，而一元线性回归仅仅只有一个自变量。那么问题来了，在线性回归中，我们是如何获得拟合曲线的呢？答案为最小二乘法，这种方法通过最小化每个观测值到估计值的离差平方和，最终确定直线的截距项与斜率，在最小二乘法中，由于所有的差距都被取了平方值，所以在相加时，我们无需考虑这个差距是正向还是负向的问题。

![](http://static.datartisan.com/upload/attachment/2015/08/0NRYqsYU.png)

![](http://static.datartisan.com/upload/attachment/2015/08/d3UVf57X.gif)

同时，我们通过可决系数R-square来评估线性回归模型的拟合优度，鉴于这是大家最为熟悉的方法，这里便不再详述了。



###2.logistic回归

logistic回归通常被用于发掘一个事件成功与否的概率，当我们的因变量是一个二元变量（0/1,真/假，是/否）时，这一方法就可以被使用。此时，模型中Y的值域为0到1之间，模型可由如下方程来表示：
```
odds= p/ (1-p) = 某事件发生的概率 / 某事件不发生的概率
ln(odds) = ln(p/(1-p))
logit(p) = ln(p/(1-p)) = b0+b1X1+b2X2+b3X3....+bkXk
```
其中，p可以是模型构建者感兴趣的任何特征出现的概率，在这儿的问题是，我们为什么要在这些式子中用到log？实际上这是因为我们需要寻找一种最为适当的连结函数形式来应对因变量所属的二项分布，并且模型中的这种形式被称为logit函数。在上面的式子中，系数的估计方法并非最小化误差平方和，而是通过样本观测值进行极大似然估计。

![](http://static.datartisan.com/upload/attachment/2015/08/MAeF9fJX.png)

###3.多项式回归

当一个回归方程的自变量的幂超过1时，我们将之称为一个多项式回归方程，例如，下式就是多项式的一个代表：
```
y=a+b*x^2
```
在这种回归技术中，最优的拟合曲线往往不是一条直线，它们通常更接近于一条观测值点之间的连线。

![](http://static.datartisan.com/upload/attachment/2015/08/dnsJfapX.png)

使用要点：



(1)当你想通过更高阶的多项式回归来减少模型误差时，可能会陷入过度拟合的陷阱之中，因此在使用这种方法时我们应该多运用图像来显示模型与观测值之间的关系，以便于更好的抓住所分析问题的本质，如下图所示：

![](http://static.datartisan.com/upload/attachment/2015/08/DaccrWHg.png)

(2)必须特别注意曲线末端的走向，以便察觉这样的曲线与趋势是否具有实际意义，过于高阶多项式在模型外推时很可能会引致一些古怪结果。
###4.逐步回归

当我们需要处理数量众多的自变量时，逐步回归将不失为一个好的回归方法。在这种回归技术中，模型对自变量的选择是一个自动化的过程，不包含任何人为干预的因素。逐步回归的这种变量选择功能往往是通过观测一系列统计指标例如R-square、t-stats和AIC来对显著变量进行识别，通常情况下，它会根据一个特定的准则一次又一次地增加或删除模型中的协变量，最终实现回归曲线的拟合。下列为逐步回归中一些最常用的方法：

（1）最基本的逐步回归只做两件事：根据需要在每个步骤中增加或删减预测因子；

（2）向前选择法：从模型中最显著的预测因子出发，逐步添加新的自变量；

（3）向后消除法：首先将所有可能的预测因子都置于模型中，之后逐步剔除最不显著的自变量。

众所周知，我们建模的目标是尽量用最少的预测变量来实现最大化的预测效率，因此逐步回归也是在我们面对高维数据集时一种可以尝试的方法。



###5.岭回归

当我们所使用的样本数据存在多重共线性问题时，岭回归是一种可行的回归方案。当多重共线性问题存在时，就连最基本的最小二乘估计都是有偏的，线性模型估计系数的方差将会很大，这表示分析结果可能远远偏离真实的水平。通过在模型中加入对偏误的度量，岭回归实现了减少模型标准误的效果。

还记得在上文中我们所提到的线性回归吧，它的模型结果通常被表示为y=a+ b*x的形式，同时这个式子中还包含了一个随机误差项，完整的模型方程表示如下：
```
y=a+b*x+e (随机误差项),  [误差项用于解释模型估计值与实际观测值之间的预测误差]
=> y=a+y= a+ b1x1+ b2x2+....+e, for multiple independent variables.
```
在线性方程中，预测误差可以被分为两个部分，第一部分来自于偏误而第二部分来自于方差，二者中的任意一个都能致使预测产生误差。在此，我们将着重分析来自于方差的那一部分误差。

为了解决多重共线性问题，岭回归通过将调整参数λ引入模型，公式如下：

![](http://static.datartisan.com/upload/attachment/2015/08/wO6aSD9G.png)

可以发现，在该式中共有两个组成部分，前者为基础的最小二乘项，而后者则为调整参数λ与系数β平方总和的乘积。对目标函数这样的补充主要用于收缩模型系数，从而起到减小模型方差的作用。

使用要点：

（1）除了正则性假设以外，这种回归在模型假设方面与最小二乘法回归完全一致；

（2）该模型收缩了估计系数的值，但是不会使它们接近于0，也就是说它并不具备变量选择的功能；

（3）该模型可以用为一种正则化方法，并且属于L2惩罚。



###6.Lasso回归

类似于岭回归，Lasso（Least Absolute Shrinkage and Selection Operator）方法也对回归系数的绝对规模采取了惩罚的形式。同时，它还能减少变异性并提高模型的精度。其目标函数如下所示：

![](http://static.datartisan.com/upload/attachment/2015/08/lrlISgWr.png)

由上式可见，Lasso回归与岭回归的不同之处在于其在惩罚函数中使用了绝对值而非平方和的形式，这将导致在模型的参数估计过程中有些系数会因为惩罚项的存在从而直接减少到0。随着惩罚力度的增强，越来越多的系数将会缩小并最终归结为0，这意味着在模型构建的同时我们也对原本给定的多个变量进行了变量选择。

使用要点：

（1）这种回归在模型假设方面与岭回归完全一致；

（2）模型可将某些变量对应的系数直接收缩至0，这有助于变量的筛选；

（3）该模型属于L1惩罚；

（4）如果存在某组预测因子高度相关的情况，Lasso方法仅会选取它们中的一个，并把所对应的系数收缩至0。



###7.弹性网（ElasticNet）回归

可以说，弹性网回归是对Lasso回归与岭回归的一种折中，在其模型训练时对L1和L2两种正则化方法进行了综合。当数据集中的许多特征都呈现出相互关联的情况时，弹性网就可以大展身手了，与Lasso相比，它不再随机选择单个变量，而是倾向于把他们全部揪出来。

![](http://static.datartisan.com/upload/attachment/2015/08/fJ4D0rIU.png)

通过这种折中的方式，弹性网络回归保留了岭回归中的一部分旋转稳定性，并因此受益。

使用要点：

（1）当存在多个相互关联的变量时，弹性网回归将加强它们之间的组效应（group effect）；

（2）该模型对选定变量的个数没有任何的限制；

（3）该模型能够承受双重的收缩。



##结语

经过上文的介绍，我们想你已经对常用的回归分析方法有了大体的认识，而与此同时，这些回归技术也应与其应用领域的数据条件息息相关。当你在针对某一具体问题选择回归分析方法时，不妨首先研究一下数据的来源与类型，考察一下他们的出处，以及是离散型还是连续型，我相信这会给你的模型选择带来实质性的帮助。

原文作者：SUNIL RAY 

翻译：SDCry!!

校对：Jude

原文链接：http://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
